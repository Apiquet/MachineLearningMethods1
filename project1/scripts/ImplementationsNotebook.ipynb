{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import numpy as np\n",
    "# Import all helper functions; packages listed explicitly for clarity.\n",
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function elementwise to input scalar or vector.\"\"\"\n",
    "    return (1/(1+np.exp(-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"Build polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        x[:,i] = (x[:,i] - np.mean(x[:,i])) / np.std(x[:,i])\n",
    "        #x[:,i] = (x[:,i] - x[:,i].min()) / (x[:,i].max() - x[:,i].min())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"Create batches for batch gradient descent algorithm.\"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate MSE value for an input error value or vector.\"\"\"\n",
    "    return (1/2)*np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Computes loss for the error calculation function in the return statement.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Execute gradient descent algorithm.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for i in range(max_iters):\n",
    "        e = y - tx.dot(w)\n",
    "        grad = - tx.T.dot(e) / len(e)\n",
    "        w = w - gamma * grad\n",
    "    loss = calculate_mse(e)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Execute stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            err = y - tx.dot(w)\n",
    "            grad = -tx.T.dot(err) / len(err)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = compute_loss(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate weights using least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ):\n",
    "   \"\"\"Calculate weights using ridge regression.\"\"\"\n",
    "   aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "   a = tx.T.dot(tx) + aI\n",
    "   b = tx.T.dot(y)\n",
    "   return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculate weights using logistic regression.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for i in range(max_iters):\n",
    "        grad = np.matmul(tx.T, sigmoid(np.matmul(tx,w)) - y)\n",
    "        w = w - gamma * grad\n",
    "    loss = grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Calculate weights using regularized logistic regression.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for i in range(max_iters):\n",
    "        grad = tx.T.dot( sigmoid(np.matmul(tx,w)) - y) + (lambda_*w)\n",
    "        w = w - gamma * grad\n",
    "    loss = grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x,i = load_csv_data('data/train.csv',sub_sample=False)\n",
    "\n",
    "x = standardize(x)\n",
    "\n",
    "# Add one dimenssion to X with only 1,beacause 1*W0+ x1*W1 + ...\n",
    "b = np.ones((x.shape[0],1), dtype = int)\n",
    "x = np.column_stack((b, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creat vector Polynomial basis\n",
    "#x = build_poly(x,31 )\n",
    "initial_w = np.random.rand(x.shape[1],1)\n",
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Choose algorithm for training.\"\"\"\n",
    "MAX_ITERS = 100\n",
    "GAMMA = 0.001\n",
    "LAMBDA_ = 0.5\n",
    "#loss, w = least_squares_GD(y,x,initial_w, MAX_ITERS, GAMMA)\n",
    "#loss , w = least_squares_SGD(y, x, initial_w, MAX_ITERS, GAMMA, LAMBDA)\n",
    "#loss, w = least_squares(y, x)\n",
    "#loss, w = logistic_regression(y, x, initial_w, MAX_ITERS, GAMMA)\n",
    "loss, w = reg_logistic_regression(y, x, initial_w, MAX_ITERS, GAMMA, LAMBDA_)\n",
    "\n",
    "#print(loss,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_accuracy(predictions, targets):\n",
    "    \"\"\"Calculate the prediction accuracy for predictions against targets.\"\"\"\n",
    "    correct = 0\n",
    "    total_samples = len(predictions)\n",
    "    for i in range(total_samples):\n",
    "        if (targets[i] == y_predictions[i]):\n",
    "            correct += 1\n",
    "    return correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accurracy: \n",
      "0.299388\n"
     ]
    }
   ],
   "source": [
    "y_predictions = predict_labels(w, x)\n",
    "\n",
    "# Generate test targets\n",
    "y_test, x, i = load_csv_data('data/test.csv',sub_sample=False)\n",
    "\n",
    "print(\"Prediction accurracy: \")\n",
    "print(calculate_prediction_accuracy(y_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
