{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import numpy as np\n",
    "# Import all helper functions; packages listed explicitly for clarity.\n",
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function elementwise to input scalar or vector.\"\"\"\n",
    "    return (1/(1+np.exp(-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"Build polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree + 1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        x[:,i] = (x[:,i] - np.mean(x[:,i])) / np.std(x[:,i])\n",
    "        #x[:,i] = (x[:,i] - x[:,i].min()) / (x[:,i].max() - x[:,i].min())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"Create batches for batch gradient descent algorithm.\"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate MSE value for an input error value or vector.\"\"\"\n",
    "    return (1/2)*np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Computes loss for the error calculation function in the return statement.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Execute gradient descent algorithm.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for i in range(max_iters):\n",
    "        e = y - tx.dot(w)\n",
    "        grad = - tx.T.dot(e) / len(e)\n",
    "        w = w - gamma * grad\n",
    "    loss = calculate_mse(e)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Execute stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            err = y - tx.dot(w)\n",
    "            grad = -tx.T.dot(err) / len(err)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = compute_loss(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate weights using least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a,b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ):\n",
    "   \"\"\"Calculate weights using ridge regression.\"\"\"\n",
    "   aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "   a = tx.T.dot(tx) + aI\n",
    "   b = tx.T.dot(y)\n",
    "   return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculate weights using logistic regression.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for i in range(max_iters):\n",
    "        grad = np.matmul(tx.T, sigmoid(np.matmul(tx,w)) - y)\n",
    "        w = w - gamma * grad\n",
    "    loss = grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Calculate weights using regularized logistic regression.\"\"\"\n",
    "    w = initial_w[:]\n",
    "    for i in range(max_iters):\n",
    "        grad = tx.T.dot( sigmoid(np.matmul(tx,w)) - y) + (lambda_*w)\n",
    "        w = w - gamma * grad\n",
    "    loss = grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(mtx):\n",
    "    \"\"\"Removes columns from the dataset where too many values are -999.\"\"\"\n",
    "    columns_to_remove = []\n",
    "    COLUMN_REMOVE_THRESHOLD = -250\n",
    "    for col_idx in range(mtx.shape[1]):\n",
    "        #print(np.sum(x[:,i])/x.shape[0])\n",
    "        if np.sum(mtx[:,col_idx])/mtx.shape[0] < COLUMN_REMOVE_THRESHOLD:\n",
    "            columns_to_remove.append(col_idx)\n",
    "    mtx_cols_removed = np.delete(mtx, columns_to_remove, axis=1)\n",
    "    return mtx_cols_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows(mtx, labels):\n",
    "    \"\"\"Removes rows from the dataset and labels where any value is -999.\"\"\"\n",
    "    rows_with_bad_data = np.where(mtx == -999)[0]\n",
    "    # Remove duplicates for rows with multiple values of -999\n",
    "    rows_to_remove = list(set(rows_with_bad_data))\n",
    "    mtx_rows_removed = np.delete(mtx, rows_to_remove, axis=0)\n",
    "    y_rows_removed = np.delete(labels, rows_to_remove)\n",
    "    return mtx_rows_removed, y_rows_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers_with_mean(mtx):\n",
    "    for col_idx in range(mtx.shape[1]):\n",
    "        col_for_calculating_mean = mtx[:,col_idx]\n",
    "        cells_with_bad_data = np.where(mtx[:,col_idx] == -999)\n",
    "        column_bad_cells_removed = np.delete(col_for_calculating_mean, cells_with_bad_data)\n",
    "        column_mean = np.mean(column_bad_cells_removed)\n",
    "\n",
    "        column_for_replacement = mtx[:,col_idx]\n",
    "        column_for_replacement[column_for_replacement == -999] = column_mean\n",
    "        mtx[:,col_idx] = column_for_replacement\n",
    "    return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1,3],[4,5],[6,-999]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4,    5],\n",
       "       [   6, -999],\n",
       "       [   1,    3]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x,i = load_csv_data('data/train.csv',sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "# Remove columns with too many -999 values\n",
    "columns_to_remove = []\n",
    "COLUMN_REMOVE_THRESHOLD = -250\n",
    "\n",
    "for col_idx in range(x.shape[1]):\n",
    "    #print(np.sum(x[:,i])/x.shape[0])\n",
    "    if np.sum(x[:,col_idx])/x.shape[0] < COLUMN_REMOVE_THRESHOLD: \n",
    "        columns_to_remove.append(col_idx)\n",
    "x_cols_removed = np.delete(x, columns_to_remove, axis=1)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211886, 20)\n"
     ]
    }
   ],
   "source": [
    "rows_with_bad_data = np.where(x_cols_removed==-999)[0]\n",
    "# remove duplicates\n",
    "rows_with_bad_data = list(set(rows_with_bad_data))\n",
    "new_data = np.delete(x_cols_removed, rows_with_bad_data, axis=0)\n",
    "print (new_data.shape)\n",
    "#x_rows_removed = np.take(x_cols_removed, list(set(np.where(x_cols_removed==-999)[0])), axis=0)\n",
    "#print(x_rows_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 2],\n",
       "       [7, 7],\n",
       "       [7, 6]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows that contain -999\n",
    "a = np.array([[8,2],[7,7],[7,6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.take(a, np.where(a==7)[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(a==7)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 7],\n",
       "       [7, 7],\n",
       "       [7, 6]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_rows_removed = np.take(a, np.where(a==7)[0], axis=0)\n",
    "a_rows_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = standardize(x)\n",
    "\n",
    "# Add one dimenssion to X with only 1,beacause 1*W0+ x1*W1 + ...\n",
    "b = np.ones((x.shape[0],1), dtype = int)\n",
    "x = np.column_stack((b, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creat vector Polynomial basis\n",
    "#x = build_poly(x,31 )\n",
    "initial_w = np.random.rand(x.shape[1],1)\n",
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Choose algorithm for training.\"\"\"\n",
    "MAX_ITERS = 100\n",
    "GAMMA = 0.001\n",
    "LAMBDA_ = 0.5\n",
    "#loss, w = least_squares_GD(y,x,initial_w, MAX_ITERS, GAMMA)\n",
    "#loss , w = least_squares_SGD(y, x, initial_w, MAX_ITERS, GAMMA, LAMBDA)\n",
    "#loss, w = least_squares(y, x)\n",
    "#loss, w = logistic_regression(y, x, initial_w, MAX_ITERS, GAMMA)\n",
    "loss, w = reg_logistic_regression(y, x, initial_w, MAX_ITERS, GAMMA, LAMBDA_)\n",
    "\n",
    "#print(loss,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_accuracy(predictions, targets):\n",
    "    \"\"\"Calculate the prediction accuracy for predictions against targets.\"\"\"\n",
    "    correct = 0\n",
    "    total_samples = len(predictions)\n",
    "    for i in range(total_samples):\n",
    "        if (targets[i] == y_predictions[i]):\n",
    "            correct += 1\n",
    "    return correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = predict_labels(w, x)\n",
    "\n",
    "# Generate test targets\n",
    "y_test, x, i = load_csv_data('data/test.csv',sub_sample=False)\n",
    "\n",
    "print(\"Prediction accurracy: \")\n",
    "print(calculate_prediction_accuracy(y_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
